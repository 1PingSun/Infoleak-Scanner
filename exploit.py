import asyncio
import aiohttp
import aiofiles
from aiohttp import ClientTimeout

PHPINFO_PATHS = [
    "phpinfo.php", "info.php", "test.php", "i.php",
    "pi.php", "php.php", "phpinfo.html"
]

GIT_PATHS = [
    ".git/HEAD", ".git/config", ".git/index"
]

OUTPUT_FILE = "scan_results.log"
MAX_CONCURRENT_TASKS = 100  # ✅ limit to 8 concurrent scans
HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; LeakScanner/1.0)"
}


async def fetch(session, url):
    try:
        async with session.get(url, timeout=5) as response:
            return response.status, await response.text(), await response.read()
    except:
        return None, "", b""


async def scan_target(session, base_url, semaphore):
    async with semaphore:  # ✅ concurrency control
        base_url = base_url.strip().rstrip("/")
        if not base_url.startswith("http"):
            base_url = "http://" + base_url

        phpinfo_found = False
        git_leak_found = False

        # Check phpinfo
        for path in PHPINFO_PATHS:
            full_url = f"{base_url}/{path}"
            status, text, _ = await fetch(session, full_url)
            if status == 200 and "phpinfo" in text.lower() and "php version" in text.lower():
                phpinfo_found = True
                break

        # Check .git leak
        for path in GIT_PATHS:
            full_url = f"{base_url}/{path}"
            status, text, content = await fetch(session, full_url)
            if status == 200 and ("ref:" in text or "[core]" in text or content.startswith(b"DIRC")):
                git_leak_found = True
                break

        result_line = f"{base_url} - phpinfo: {'YES' if phpinfo_found else 'NO'} - git leak: {'YES' if git_leak_found else 'NO'}"
        print(result_line)

        # Write to log file safely
        async with aiofiles.open(OUTPUT_FILE, mode="a") as f:
            await f.write(result_line + "\n")


async def main():
    # Clear previous log
    async with aiofiles.open(OUTPUT_FILE, mode="w") as f:
        await f.write("")

    # Load target URLs
    async with aiofiles.open("urls.txt", mode="r") as f:
        targets = await f.readlines()

    semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)

    async with aiohttp.ClientSession(headers=HEADERS, timeout=ClientTimeout(total=10)) as session:
        tasks = [scan_target(session, url, semaphore) for url in targets if url.strip()]
        await asyncio.gather(*tasks)


if __name__ == "__main__":
    asyncio.run(main())
